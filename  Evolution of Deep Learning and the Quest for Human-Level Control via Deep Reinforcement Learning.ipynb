{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c70847d-a083-4488-9fd8-85a2d6a76697",
   "metadata": {},
   "source": [
    "# A Journey through the Evolution of Deep Learning and the Quest for Human-Level Control via Deep Reinforcement Learning\n",
    "\n",
    "### By Atanas Vasev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842edccb-153b-41b7-ba4a-5e7bb1d0243b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26063fea-9263-4f9b-9730-0e0f94bb75c7",
   "metadata": {},
   "source": [
    "Despite the seemingly futuristic and complex nature of the term 'Deep Learning,' its roots can be traced back to 1940 with the pioneering work of Warren McCulloch and Walter Pitts. They laid the foundation for neural networks with their influential paper titled 'A Logical Calculus of Ideas Immanent in Nervous Activity.' However, progress in neural networks faced challenges for decades due to limitations in computing power and data availability. It wasn't until the early 21st century that significant advancements reignited interest, with a transformative moment occurring in 2012 during the ImageNet Large Scale Visual Recognition Challenge.\n",
    "\n",
    "This pivotal moment was characterized by the breakthrough achieved by a team led by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. They demonstrated a groundbreaking leap in image classification accuracy using a deep neural network architecture known as AlexNet. This success marked the resurgence of deep learning, showcasing its superior capabilities in handling complex tasks.\n",
    "\n",
    "The success of AlexNet opened the door for extensive exploration and development in deep neural networks. Researchers delved into designing more sophisticated architectures, introducing techniques like convolutional layers for image processing. This momentum catalyzed breakthroughs in various domains, including natural language processing and speech recognition.\n",
    "\n",
    "The landscape of deep learning further evolved with the introduction of reinforcement learning techniques, notably highlighted by DeepMind's Deep Q-Learning algorithm. In particular, Deep Q-Learning garnered attention with its remarkable performance in the game of Breakout. DeepMind demonstrated the ability of the algorithm to achieve superhuman performance by learning optimal strategies through reinforcement learning.\n",
    "\n",
    "As researchers progressed, they continued to refine and expand reinforcement learning methodologies. The focus on enhancing training techniques and algorithmic advancements led to further breakthroughs, making reinforcement learning a key player in the evolution of deep learning.\n",
    "\n",
    "The dynamic journey of deep learning, from the foundational work of McCulloch and Pitts to the complex, versatile neural networks of today, is exemplified by the continuous exploration of novel architectures, training methodologies, and applications. In the realm of reinforcement learning, the impact of algorithms like Deep Q-Learning serves as a testament to the ongoing commitment to unraveling the full potential of deep learning in replicating and surpassing human-level control, particularly in challenging domains such as Breakout.\n",
    "\n",
    "In this paper, I will attempt to replicate DeepMind's Deep Q-Learning algorithm. The primary objective is to achieve a greater understanding not only of reinforcement learning but also of the broader topic of deep learning. The reason I chose this particular paper and topic is that this type of deep learning architecture can serve as a valuable case study for its success in achieving human-level performance in complex tasks. Deep Q-Learning, proposed by researchers at DeepMind, has demonstrated remarkable capabilities in mastering complex environments through reinforcement learning. By replicating and dissecting the nuances of this algorithm, I aim to unravel insights into the principles of reinforcement learning and the intricate mechanisms that contribute to the success of deep learning models.\n",
    "\n",
    "The journey involves implementing, testing, and fine-tuning the Deep Q-Learning algorithm, aiming not only for successful replication but also for a deeper comprehension of the underlying principles. Furthermore, this endeavor provides an opportunity to explore the broader implications of deep learning, such as its adaptability to different domains and its potential for scalable and efficient learning.\n",
    "\n",
    "As the paper progresses, I will delve into the historical context of deep learning, tracing its evolution from its early theoretical foundations to the recent breakthroughs in algorithmic advancements. By synthesizing this historical narrative with practical implementation, the aim is to provide a comprehensive exploration of deep learning, emphasizing its transformative impact on artificial intelligence.\n",
    "\n",
    "Through the lens of replicating Deep Q-Learning, this paper seeks to contribute to the ongoing discourse on the challenges, advancements, and future prospects of deep learning. By gaining insights into the inner workings of this algorithm, we can further our understanding of the broader implications of reinforcement learning and its potential applications in real-world scenarios.\n",
    "\n",
    "In 2013 a London based startup called DeepMind published a groundbreaking paper called [\"Playing Atari with Deep Reinforcement Learning\"](https://arxiv.org/abs/1312.5602) on arXiv: The authors presented a variant of Reinforcement Learning called Deep Q-Learning that is able to successfully learn control policies for different Atari 2600 games receiving only screen pixels as input and a reward when the game score changes. The agent even surpasses human expert players in some of those games! This is an astonishing result because previously “AIs” used to be limited to one single game, for instance, chess, whereas in this case the types and contents of the games in the Arcade Learning Environment vary significantly and yet no adjustment of the architecture, learning algorithm or hyperparameters is needed.\n",
    "\n",
    "No wonder DeepMind was bought by Google for 500 Million Dollars. The company has since been one of the leading institutions advancing Deep Learning research and a later article discussing DQN(Deep Q-Networks) has been published in Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5cabb-6535-41e3-80f2-2ee7b3c26cb2",
   "metadata": {},
   "source": [
    "# 2. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41545143-96ee-4fdd-b7ad-b3c6e2d41bd5",
   "metadata": {},
   "source": [
    "Before we start the implementation we need to cover some important concepts in the field of reinforcement learning: Q-learning and deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e6f74-ef91-4709-bba5-64630adedde4",
   "metadata": {},
   "source": [
    "### 2.1 Reinforcement Learning\n",
    "A key differentiator of reinforcement learning from supervised or unsupervised learning is the presence of two things:\n",
    "\n",
    " * An environment: This could be something like a maze, a video game, the stock market, and so on.\n",
    " * An agent: This is the AI that learns how to operate and succeed in a given environment\n",
    "\n",
    "The way the agent learns how to operate in the environment is through an iterative feedback loop. The agent first takes an action and as a result, the state will change based on the rewards either won or lost.\n",
    "\n",
    "**The goal of the agent is to maximize its cumulative expected reward.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0917fd-1137-4476-8ae9-5a3f8c5a0ce6",
   "metadata": {},
   "source": [
    "### 2.2 Bellman Equation\n",
    "The [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation?ref=mlq.ai) is a fundamental concept in reinforcement learning.\n",
    "\n",
    "The concepts we need to understand the Bellman Equation include:\n",
    "\n",
    "$s$ - State\n",
    "\n",
    "$a$ - Action\n",
    "\n",
    "$R$ - Reward\n",
    "\n",
    "$γ$ - Discount factor\n",
    "\n",
    "The Bellman Equation was introduced by Dr. Richard Bellman (who's known as the Father of dynamic programming) in 1954 in the paper: [The Theory of Dynamic Programming](https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf?ref=mlq.ai).\n",
    "\n",
    "To understand this, let's look at an example from freeCodeCamp, in which we ask the question:\n",
    "\n",
    "**How do we train a robot to reach the end goal with the shortest path without stepping on a mine?**\n",
    "\n",
    "<img src=\"./IMG/Robot_Maze.png\" width=\"300\">\n",
    "\n",
    "Below is the reward system for this environment:\n",
    "\n",
    " - 1 point at each step. This is to encourage the agent to reach the goal in the shortest path.\n",
    "   \n",
    " - 100 for stepping on a mind and the game ends\n",
    "   \n",
    " + 1 for landing on a⚡️\n",
    " + \n",
    " + 100 for reaching the End\n",
    "\n",
    "To solve this we introduce the concept of a Q-table.\n",
    "\n",
    "A Q-table is a lookup table that calculates the expected future rewards for each action in each state. This lets the agent choose the best action in each state.\n",
    "\n",
    "In this example, our agent has 4 actions (up, down, left, right) and 5 possible states (Start, Blank, Lightning, Mine, End).\n",
    "\n",
    "So the question is: how do we calculate the maximum expected reward in each state, or the values of the Q-table?\n",
    "\n",
    "We learn the value of the Q-table through an iterative process using the Q-learning algorithm, which uses the Bellman Equation.\n",
    "\n",
    "Here is the Bellman equation for deterministic environments:\n",
    "\n",
    "$V(s)=max_aR(s,a)+γV(s′))$\n",
    "\n",
    "Here's a summary of the equation from our earlier Guide to Reinforcement Learning:\n",
    "\n",
    "* The value of a given state is equal to max action, which means of all the available actions in the state we're in, we pick the one that maximizes value.\n",
    "\n",
    "* We take the reward of the optimal action $a$ in state $s$ and add a multiplier of $γ$, which is the discount factor that diminishes our reward over time.\n",
    "\n",
    "* Each time we take an action we get back the next state: $s'$. This is where dynamic programming comes in, since it is recursive we take $s$ and put it back into the value function $V(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94967dc8-2d02-47e7-8abd-41171e57cb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Markov Decision Processes (MDPs)\n",
    "\n",
    "Now that we've discussed the concept of a Q-table, let's move on to the next key concept in reinforcement learning: Markov decision processes, or MDPs.\n",
    "\n",
    "First let's review the difference between deterministic and non-deterministic search.\n",
    "\n",
    "Deterministic Search:\n",
    " * Deterministic search means that if the agent tries to go up (in our maze example), then with 100% probability it will in fact go up.\n",
    "\n",
    "Non-Deterministic Search:\n",
    "*  Non-deterministic search means that if our agent wants to up, there could be an 80% chance it goes up, a 10% it goes left, and 10% it goes right, for example. So there is an element of randomness in the environment that we need to account for this.\n",
    "\n",
    "This is where two new concepts come in: Markov processes and Markov decision processes.\n",
    "\n",
    "Here's the definition of [Markov process](https://en.wikipedia.org/wiki/Markov_property?ref=mlq.ai) from Wikipedia:\n",
    "\n",
    " **\"A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.\"**\n",
    "\n",
    "To simplify this, in an environment with a Markov property the way the environment is designed in such a way that what happens in the future doesn't depend on the past.\n",
    "\n",
    "Now here's the definition of a [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process?ref=mlq.ai):\n",
    "\n",
    "**\"A Markov decision process provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\"**\n",
    "\n",
    "A Markov decision process is the framework that the agent will use in order to operate in this partly random environment.\n",
    "\n",
    "Let's now build on our Bellman equation, which as a reminder is:\n",
    "\n",
    "$V(s)=max_aR(s,a)+γV(s′))$\n",
    "\n",
    "Now that we have some randomness we don't actually know what $s′$ we'll end up in. Instead, we have to use the expected value of the next state.\n",
    "\n",
    "To do this, we would multiply our three possible states by their probability. Now our expected value would be:\n",
    "\n",
    "$0.8∗V(s′1)+0.1∗V(s′2)+0.1∗V(s3)$ <font size=\"1\">**NB: the numbers are taken from the example for Non-Deterministic Search**</font>\n",
    "\n",
    "Now the new Bellman equation for non-deterministic environments is:\n",
    "\n",
    "$V_(s)=max_a(R(s,a)+γ∑s′P(s,a,s′)V(s′))$\n",
    "\n",
    "This equation is what we'll be dealing with going forward since most realistic environments are stochastic in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298b09a-6594-48c9-904f-a6f635c2907d",
   "metadata": {},
   "source": [
    "### 2.4 Q-Learning Intuition\n",
    "\n",
    "Now that we understand the Bellman equation and understand a Markov decision process for the probability of the next state given an action, let's move on to Q-learning.\n",
    "\n",
    "So far we've been dealing with the value of being in a given state, and we know we want to make an optimal decision about where to go next given our current state.\n",
    "\n",
    "Now instead of looking at the value of each state $V(s)$, we're going to look at the value of each state-action pair, which is denoted by $Q(s,a)$.\n",
    "\n",
    "Intuitively you can think of the Q-value as the quality of each action.Let's look at how we actually derive the value of $Q(s,a)$\n",
    "by comparing is to $V(s)$.\n",
    "\n",
    "As we just saw, here is the equation for $V(s)$\n",
    "\n",
    "in a stochastic environment:\n",
    "\n",
    "$$V_(s)=max_a(R(s,a)+γ∑s′P(s,a,s′)V(s′))$$\n",
    "\n",
    "Here is the equation for $Q(s,a)$:\n",
    "\n",
    "* By performing an action the first thing we get is a reward $R(s,a)$\n",
    "\n",
    "* Now the agent is in the next state $s′$, and because the agent can end up in several states, we add the value of the next state which is the expected value of the next state \n",
    "\n",
    "$$Q(s,a)=R(s,a)+γ∑s′P(s,a,s′)V(s′))$$\n",
    "\n",
    "What you will notice looking at this equation is that $Q(s,a)$ is the same value as what's in the brackets of the Bellman equation: $R(s,a)+γ∑s′P(s,a,s′)V(s′))$.\n",
    "\n",
    "Here's how we can think about this intuitively:\n",
    "\n",
    "The value of a state $V(s)$ is the maximum of all the possible Q-values.\n",
    "\n",
    "Let's take this equation a step further by getting rid of $V$, since $V$ is a recursive function of $V$.\n",
    "\n",
    "We're going to take the $V(s′)$ and replace it with $Q(s′,a′)$:\n",
    "\n",
    "$$Q(s,a)=R(s,a)+γ∑s′P(s,a,s′)max_a′Q(s′,a′))$$\n",
    "\n",
    "Now we have a recursive formula for the Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8826d7-8f75-4c9b-996c-9ca68f598b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.5 Temporal Difference\n",
    "\n",
    "Temporal difference is an important concept at the heart of the Q-learning algorithm. This is how everything we've learned so far comes together in Q-learning. One thing we haven't mentioned yet about non-deterministic search is that it can be very difficult to actually calculate the value of each state.\n",
    "\n",
    "Temporal difference is what allows us to calculate these values.\n",
    "\n",
    "For now, we're just going to use the deterministic Bellman equation for simplicity, which to recap is: $Q(s,a)=R(s,a)+γmax_a′Q(s′,a′)$, but we'll use this refer to stochastic environments.\n",
    "\n",
    "So we know that before an agent takes an action it has a Q-value: $Q(s,a)$.\n",
    "After an action is taken we know what reward the agent actually got and what the value of the new state is: $R(s,a)+γmax_a′Q(s′,a′)$.\n",
    "\n",
    "The temporal difference is defined as follows:\n",
    "\n",
    "$TD(a,s)=R(s,a)+γmaxa′Q(s′,a′)−Qt−1(s,a)$\n",
    "\n",
    "The first element is what we get after taking an action and the second element is the previous Q-value.\n",
    "\n",
    "The question is are these two values the same?\n",
    "\n",
    "Ideally, these two values should be the same since the first, $R(s,a)+γmax_a′Q(s′,a′)$ is the formula for calculating $Q(s,a)$.\n",
    "But these values may not be the same because of the randomness that exists in the environment.\n",
    "\n",
    "The reason it's called temporal difference is because of time. We have $Q(s,a)$ which is our previous Q-value and we have our new Q-value, which is $R(s,a)+γmaxa′Q(s′,a′)$.\n",
    "\n",
    "So the question is: has there been a difference between these values in time?\n",
    "\n",
    "Now that we have our temporal difference, here's how we use it:\n",
    "\n",
    "$Qt(s,a)=Qt−1(s,a)+αTDt(a,s)$\n",
    "\n",
    "where:\n",
    "\n",
    "$α$ - is our learning rate.\n",
    "\n",
    "So we take our previous $Qt−1(s,a)$ and add on the temporal difference times the learning rate to get our new $Qt(s,a)$.\n",
    "This equation is how our Q-values are updated over time. Let's now plug in the $TD(a,s)$ equation into our new Q-learning equation:\n",
    "\n",
    "$Qt(s,a)=Qt−1(s,a)+α(R(s,a)+γmax_a′Q(s′,a′)−Qt−1(s,a))$\n",
    "\n",
    "We now have the full Q-learning equation, so let's move on to deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29597c05-0861-4e27-a6b8-02e290aa73a9",
   "metadata": {},
   "source": [
    "### 2.6 Deep Q-Learning Intuition\n",
    "\n",
    "In deep Q-learning we are, of course, making use of neural networks. In terms of the neural network we feed in the state, pass that through several hidden layers (the exact number depends on the architecture) and then output the Q-values. Here is a good visual representation of Q-learning vs. deep Q-learning from Analytics Vidhya:\n",
    "\n",
    "<img src=\"./IMG/Q-learining_vs_Deep_Q-learning.webp\" width=\"600\">\n",
    "\n",
    "You may be wondering why we need to introduce deep learning to the Q-learning equation. Q-learning works well when we have a relatively simple environment to solve, but when the number of states and actions we can take gets more complex we use deep learning as a function approximator. Let's look at how the equation changes with deep Q-learning.\n",
    "\n",
    "Recall the equation for temporal difference:\n",
    "\n",
    "$TD(a,s)=R(s,a)+γmax_a′Q(s′,a′)−Qt−1(s,a)$\n",
    "\n",
    "In the maze example, the neural network will predict 4 values: up, right, left, or down.We then take these 4 values and compare it to the values that were previously predicted, which are stored in memory.\n",
    "\n",
    "So we're comparing Q1 vs. Q−Target1, Q2 vs. Q−Target2, etc.\n",
    "Recall that neural networks work by updating their weights, so we need to adapt our temporal difference equation to leverage this. So what we're going to do is calculate a loss by taking the sum of the squared differences of the Q-values and their targets:\n",
    "\n",
    "$L=∑(Q−Target−Q)^2$.\n",
    "\n",
    "We then take this loss and use backpropagation, or stochastic gradient descent, and pass it through the network and update the weights. This is the learning part, now let's move on to how the agent selects the best action to take.To choose which action is the best, we use the Q-values that we have and pass them through a softmax function. This process happens every time the agent is in a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98f3df-14b5-43c2-ac53-03df8244359b",
   "metadata": {},
   "source": [
    "### 2.7 Experience Replay\n",
    "\n",
    "Now that we've discussed how to apply neural networks to Q-learning, let's review another important concept in deep Q-learning: experience replay.\n",
    "One thing that can lead to our agent misunderstanding the environment is consecutive interdependent states that are very similar.\n",
    "\n",
    "For example, if we're teaching a self-driving car how to drive, and the first part of the road is just a straight line, the agent might not learn how to deal with any curves in the road.\n",
    "This is where experience replay comes in.\n",
    "\n",
    "From our self-driving car example, what happens with experience replay is that the initial experiences of driving in a straight line don't get put through the neural network right away.\n",
    "Instead, these experiences are saved into memory by the agent.\n",
    "Once the agent reaches a certain threshold then we tell the agent to learn from it.\n",
    "\n",
    "So the agent is now learning from a batch of experiences. From these experiences, the agent randomly selects a uniformly distributed sample from this batch and learns from that.\n",
    "\n",
    "Each experience is characterized by that state it was in, the action it took, the state it ended up in, and the reward it received.\n",
    "By randomly sampling from the experiences, this breaks the bias that may have come from the sequential nature of a particular environment, for example driving in a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b965c97-4f32-45c7-917e-4fd56acd0450",
   "metadata": {},
   "source": [
    "### 2.8 Action Selection Policies\n",
    "\n",
    "Now that we understand how deep Q-learning uses experience replay to learn from a batch of experiences stored in memory, let's finish off with how the agent actually selects an action.\n",
    "\n",
    "So the question is: once we have the Q-values, how do decide which one to use?\n",
    "Recall that in simple Q-learning we just choose the action with the highest Q-value.\n",
    "\n",
    "As we mentioned earlier, with deep Q-learning we pass the Q-values through a softmax function.\n",
    "In reality, the action selection policy doesn't need to be softmax, there are others that could be used, and a few of the most common include:\n",
    "\n",
    "* $ϵ-greedy$\n",
    "\n",
    "* $ϵ-soft$\n",
    "\n",
    "* Softmax\n",
    "\n",
    "The reason that we don't just use the highest Q-value comes down to an important concept in reinforcement learning: the exploration vs. exploitation dilemma.\n",
    "\n",
    "To summarize this concept, an agent must make a tradeoff between taking advantage of what it already knows about the environment, or exploring further.\n",
    "If it takes advantage of what it already knows it could gain more rewards, but if it doesn't explore further it may not be choosing the optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889deafe-905d-4be7-8e09-7570ac9d3c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Implementation of Deep Q-Learning and how to match DeepMind’s score in Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696fb52-23d0-4b8d-8ca5-1af1d0017a53",
   "metadata": {},
   "source": [
    "We are going to explore the method of implementing Deep Q-Networks (DQN) by following the outlined roadmap and explore it problems and resolved them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997113e9-5009-482c-a115-feff9b7ba8bf",
   "metadata": {},
   "source": [
    "### 3.1. Reinfocement Learning in Deep Q-Learning\n",
    "\n",
    "In supervised learning, a neural network learns to map inputs to outputs based on labeled training data. For instance, in classifying cats and dogs, the network is trained with labeled images until it can accurately classify new images.\n",
    "\n",
    "In reinforcement learning, the approach is more akin to a child learning a new game. The computer explores the environment, receives occasional rewards when the score increases, and learns to make decisions by evaluating actions that lead to higher scores.\n",
    "\n",
    "In contrast to supervised learning, reinforcement learning (RL) faces challenges with \"sparse\" and \"delayed\" rewards, as mentioned by [Mnih et al. 2013](https://arxiv.org/abs/1312.5602). \"Sparse\" implies infrequent rewards, and \"delayed\" refers to the significant time gap between an action and the corresponding reward. In RL scenarios, the agent might not receive feedback immediately after making a decision, making it challenging to associate actions with their eventual outcomes. For instance, in a maze, choosing a path at a fork may not yield immediate feedback; the reward, finding gold, comes later. To address this, RL uses a discount factor $\\gamma$ to prioritize immediate rewards over delayed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7bf1f-bf9c-4b27-a33a-7352d0460d29",
   "metadata": {},
   "source": [
    "# Refferences\n",
    "[1] How to match DeepMind’s Deep Q-Learning score in Breakout by Fabio M. Graetz, Aug 26, 2018\n",
    "\n",
    "[2] Deep Reinforcement Learning: Guide to Deep Q-Learning by Peter Foy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
