{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1d5362-d4c6-4d42-95dd-1da9f5cc84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70847d-a083-4488-9fd8-85a2d6a76697",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Journey through the Evolution of Deep Learning and the Quest for Human-Level Control via Deep Reinforcement Learning\n",
    "\n",
    "### By Atanas Vasev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842edccb-153b-41b7-ba4a-5e7bb1d0243b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26063fea-9263-4f9b-9730-0e0f94bb75c7",
   "metadata": {},
   "source": [
    "Despite the seemingly futuristic and complex nature of the term 'Deep Learning,' its roots can be traced back to 1940 with the pioneering work of Warren McCulloch and Walter Pitts. They laid the foundation for neural networks with their influential paper titled 'A Logical Calculus of Ideas Immanent in Nervous Activity.' However, progress in neural networks faced challenges for decades due to limitations in computing power and data availability. It wasn't until the early 21st century that significant advancements reignited interest, with a transformative moment occurring in 2012 during the ImageNet Large Scale Visual Recognition Challenge.\n",
    "\n",
    "This pivotal moment was characterized by the breakthrough achieved by a team led by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. They demonstrated a groundbreaking leap in image classification accuracy using a deep neural network architecture known as AlexNet. This success marked the resurgence of deep learning, showcasing its superior capabilities in handling complex tasks.\n",
    "\n",
    "The success of AlexNet opened the door for extensive exploration and development in deep neural networks. Researchers delved into designing more sophisticated architectures, introducing techniques like convolutional layers for image processing. This momentum catalyzed breakthroughs in various domains, including natural language processing and speech recognition.\n",
    "\n",
    "The landscape of deep learning further evolved with the introduction of reinforcement learning techniques, notably highlighted by DeepMind's Deep Q-Learning algorithm. In particular, Deep Q-Learning garnered attention with its remarkable performance in the game of Breakout. DeepMind demonstrated the ability of the algorithm to achieve superhuman performance by learning optimal strategies through reinforcement learning.\n",
    "\n",
    "As researchers progressed, they continued to refine and expand reinforcement learning methodologies. The focus on enhancing training techniques and algorithmic advancements led to further breakthroughs, making reinforcement learning a key player in the evolution of deep learning.\n",
    "\n",
    "The dynamic journey of deep learning, from the foundational work of McCulloch and Pitts to the complex, versatile neural networks of today, is exemplified by the continuous exploration of novel architectures, training methodologies, and applications. In the realm of reinforcement learning, the impact of algorithms like Deep Q-Learning serves as a testament to the ongoing commitment to unraveling the full potential of deep learning in replicating and surpassing human-level control, particularly in challenging domains such as Breakout.\n",
    "\n",
    "In this paper, I will attempt to replicate DeepMind's Deep Q-Learning algorithm. The primary objective is to achieve a greater understanding not only of reinforcement learning but also of the broader topic of deep learning. The reason I chose this particular paper and topic is that this type of deep learning architecture can serve as a valuable case study for its success in achieving human-level performance in complex tasks. Deep Q-Learning, proposed by researchers at DeepMind, has demonstrated remarkable capabilities in mastering complex environments through reinforcement learning. By replicating and dissecting the nuances of this algorithm, I aim to unravel insights into the principles of reinforcement learning and the intricate mechanisms that contribute to the success of deep learning models.\n",
    "\n",
    "The journey involves implementing, testing, and fine-tuning the Deep Q-Learning algorithm, aiming not only for successful replication but also for a deeper comprehension of the underlying principles. Furthermore, this endeavor provides an opportunity to explore the broader implications of deep learning, such as its adaptability to different domains and its potential for scalable and efficient learning.\n",
    "\n",
    "As the paper progresses, I will delve into the historical context of deep learning, tracing its evolution from its early theoretical foundations to the recent breakthroughs in algorithmic advancements. By synthesizing this historical narrative with practical implementation, the aim is to provide a comprehensive exploration of deep learning, emphasizing its transformative impact on artificial intelligence.\n",
    "\n",
    "Through the lens of replicating Deep Q-Learning, this paper seeks to contribute to the ongoing discourse on the challenges, advancements, and future prospects of deep learning. By gaining insights into the inner workings of this algorithm, we can further our understanding of the broader implications of reinforcement learning and its potential applications in real-world scenarios.\n",
    "\n",
    "In 2013 a London based startup called DeepMind published a groundbreaking paper called [\"Playing Atari with Deep Reinforcement Learning\"](https://arxiv.org/abs/1312.5602) on arXiv: The authors presented a variant of Reinforcement Learning called Deep Q-Learning that is able to successfully learn control policies for different Atari 2600 games receiving only screen pixels as input and a reward when the game score changes. The agent even surpasses human expert players in some of those games! This is an astonishing result because previously “AIs” used to be limited to one single game, for instance, chess, whereas in this case the types and contents of the games in the Arcade Learning Environment vary significantly and yet no adjustment of the architecture, learning algorithm or hyperparameters is needed.\n",
    "\n",
    "No wonder DeepMind was bought by Google for 500 Million Dollars. The company has since been one of the leading institutions advancing Deep Learning research and a later article discussing DQN(Deep Q-Networks) has been published in Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5cabb-6535-41e3-80f2-2ee7b3c26cb2",
   "metadata": {},
   "source": [
    "# 2. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41545143-96ee-4fdd-b7ad-b3c6e2d41bd5",
   "metadata": {},
   "source": [
    "Before we start the implementation we need to cover some important concepts in the field of reinforcement learning: Q-learning and deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e6f74-ef91-4709-bba5-64630adedde4",
   "metadata": {},
   "source": [
    "### 2.1 Reinforcement Learning\n",
    "A key differentiator of reinforcement learning from supervised or unsupervised learning is the presence of two things:\n",
    "\n",
    " * An environment: This could be something like a maze, a video game, the stock market, and so on.\n",
    " * An agent: This is the AI that learns how to operate and succeed in a given environment\n",
    "\n",
    "The way the agent learns how to operate in the environment is through an iterative feedback loop. The agent first takes an action and as a result, the state will change based on the rewards either won or lost.\n",
    "\n",
    "**The goal of the agent is to maximize its cumulative expected reward.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0917fd-1137-4476-8ae9-5a3f8c5a0ce6",
   "metadata": {},
   "source": [
    "### 2.2 Bellman Equation\n",
    "The [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation?ref=mlq.ai) is a fundamental concept in reinforcement learning.\n",
    "\n",
    "The concepts we need to understand the Bellman Equation include:\n",
    "\n",
    "$s$ - State\n",
    "\n",
    "$a$ - Action\n",
    "\n",
    "$R$ - Reward\n",
    "\n",
    "$γ$ - Discount factor\n",
    "\n",
    "The Bellman Equation was introduced by Dr. Richard Bellman (who's known as the Father of dynamic programming) in 1954 in the paper: [The Theory of Dynamic Programming](https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf?ref=mlq.ai).\n",
    "\n",
    "To understand this, let's look at an example from freeCodeCamp, in which we ask the question:\n",
    "\n",
    "**How do we train a robot to reach the end goal with the shortest path without stepping on a mine?**\n",
    "\n",
    "<img src=\"./IMG/Robot_Maze.png\" width=\"300\">\n",
    "\n",
    "Below is the reward system for this environment:\n",
    "\n",
    " - 1 point at each step. This is to encourage the agent to reach the goal in the shortest path.\n",
    "   \n",
    " - 100 for stepping on a mind and the game ends\n",
    "   \n",
    " + 1 for landing on a⚡️\n",
    " + \n",
    " + 100 for reaching the End\n",
    "\n",
    "To solve this we introduce the concept of a Q-table.\n",
    "\n",
    "A Q-table is a lookup table that calculates the expected future rewards for each action in each state. This lets the agent choose the best action in each state.\n",
    "\n",
    "In this example, our agent has 4 actions (up, down, left, right) and 5 possible states (Start, Blank, Lightning, Mine, End).\n",
    "\n",
    "So the question is: how do we calculate the maximum expected reward in each state, or the values of the Q-table?\n",
    "\n",
    "We learn the value of the Q-table through an iterative process using the Q-learning algorithm, which uses the Bellman Equation.\n",
    "\n",
    "Here is the Bellman equation for deterministic environments:\n",
    "\n",
    "$V(s)=max_aR(s,a)+γV(s′))$\n",
    "\n",
    "Here's a summary of the equation from our earlier Guide to Reinforcement Learning:\n",
    "\n",
    "* The value of a given state is equal to max action, which means of all the available actions in the state we're in, we pick the one that maximizes value.\n",
    "\n",
    "* We take the reward of the optimal action $a$ in state $s$ and add a multiplier of $γ$, which is the discount factor that diminishes our reward over time.\n",
    "\n",
    "* Each time we take an action we get back the next state: $s'$. This is where dynamic programming comes in, since it is recursive we take $s$ and put it back into the value function $V(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94967dc8-2d02-47e7-8abd-41171e57cb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Markov Decision Processes (MDPs)\n",
    "\n",
    "Now that we've discussed the concept of a Q-table, let's move on to the next key concept in reinforcement learning: Markov decision processes, or MDPs.\n",
    "\n",
    "First let's review the difference between deterministic and non-deterministic search.\n",
    "\n",
    "Deterministic Search:\n",
    " * Deterministic search means that if the agent tries to go up (in our maze example), then with 100% probability it will in fact go up.\n",
    "\n",
    "Non-Deterministic Search:\n",
    "*  Non-deterministic search means that if our agent wants to up, there could be an 80% chance it goes up, a 10% it goes left, and 10% it goes right, for example. So there is an element of randomness in the environment that we need to account for this.\n",
    "\n",
    "This is where two new concepts come in: Markov processes and Markov decision processes.\n",
    "\n",
    "Here's the definition of [Markov process](https://en.wikipedia.org/wiki/Markov_property?ref=mlq.ai) from Wikipedia:\n",
    "\n",
    " **\"A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.\"**\n",
    "\n",
    "To simplify this, in an environment with a Markov property the way the environment is designed in such a way that what happens in the future doesn't depend on the past.\n",
    "\n",
    "Now here's the definition of a [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process?ref=mlq.ai):\n",
    "\n",
    "**\"A Markov decision process provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\"**\n",
    "\n",
    "A Markov decision process is the framework that the agent will use in order to operate in this partly random environment.\n",
    "\n",
    "Let's now build on our Bellman equation, which as a reminder is:\n",
    "\n",
    "$V(s)=max_aR(s,a)+γV(s′))$\n",
    "\n",
    "Now that we have some randomness we don't actually know what $s′$ we'll end up in. Instead, we have to use the expected value of the next state.\n",
    "\n",
    "To do this, we would multiply our three possible states by their probability. Now our expected value would be:\n",
    "\n",
    "$0.8∗V(s′1)+0.1∗V(s′2)+0.1∗V(s3)$ <font size=\"1\">**NB: the numbers are taken from the example for Non-Deterministic Search**</font>\n",
    "\n",
    "Now the new Bellman equation for non-deterministic environments is:\n",
    "\n",
    "$V_(s)=max_a(R(s,a)+γ∑s′P(s,a,s′)V(s′))$\n",
    "\n",
    "This equation is what we'll be dealing with going forward since most realistic environments are stochastic in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298b09a-6594-48c9-904f-a6f635c2907d",
   "metadata": {},
   "source": [
    "### 2.4 Q-Learning Intuition\n",
    "\n",
    "Now that we understand the Bellman equation and understand a Markov decision process for the probability of the next state given an action, let's move on to Q-learning.\n",
    "\n",
    "So far we've been dealing with the value of being in a given state, and we know we want to make an optimal decision about where to go next given our current state.\n",
    "\n",
    "Now instead of looking at the value of each state $V(s)$, we're going to look at the value of each state-action pair, which is denoted by $Q(s,a)$.\n",
    "\n",
    "Intuitively you can think of the Q-value as the quality of each action.Let's look at how we actually derive the value of $Q(s,a)$\n",
    "by comparing is to $V(s)$.\n",
    "\n",
    "As we just saw, here is the equation for $V(s)$\n",
    "\n",
    "in a stochastic environment:\n",
    "\n",
    "$$V_(s)=max_a(R(s,a)+γ∑s′P(s,a,s′)V(s′))$$\n",
    "\n",
    "Here is the equation for $Q(s,a)$:\n",
    "\n",
    "* By performing an action the first thing we get is a reward $R(s,a)$\n",
    "\n",
    "* Now the agent is in the next state $s′$, and because the agent can end up in several states, we add the value of the next state which is the expected value of the next state \n",
    "\n",
    "$$Q(s,a)=R(s,a)+γ∑s′P(s,a,s′)V(s′))$$\n",
    "\n",
    "What you will notice looking at this equation is that $Q(s,a)$ is the same value as what's in the brackets of the Bellman equation: $R(s,a)+γ∑s′P(s,a,s′)V(s′))$.\n",
    "\n",
    "Here's how we can think about this intuitively:\n",
    "\n",
    "The value of a state $V(s)$ is the maximum of all the possible Q-values.\n",
    "\n",
    "Let's take this equation a step further by getting rid of $V$, since $V$ is a recursive function of $V$.\n",
    "\n",
    "We're going to take the $V(s′)$ and replace it with $Q(s′,a′)$:\n",
    "\n",
    "$$Q(s,a)=R(s,a)+γ∑s′P(s,a,s′)max_a′Q(s′,a′))$$\n",
    "\n",
    "Now we have a recursive formula for the Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8826d7-8f75-4c9b-996c-9ca68f598b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.5 Temporal Difference\n",
    "\n",
    "Temporal difference is an important concept at the heart of the Q-learning algorithm. This is how everything we've learned so far comes together in Q-learning. One thing we haven't mentioned yet about non-deterministic search is that it can be very difficult to actually calculate the value of each state.\n",
    "\n",
    "Temporal difference is what allows us to calculate these values.\n",
    "\n",
    "For now, we're just going to use the deterministic Bellman equation for simplicity, which to recap is: $Q(s,a)=R(s,a)+γmax_a′Q(s′,a′)$, but we'll use this refer to stochastic environments.\n",
    "\n",
    "So we know that before an agent takes an action it has a Q-value: $Q(s,a)$.\n",
    "After an action is taken we know what reward the agent actually got and what the value of the new state is: $R(s,a)+γmax_a′Q(s′,a′)$.\n",
    "\n",
    "The temporal difference is defined as follows:\n",
    "\n",
    "$TD(a,s)=R(s,a)+γmaxa′Q(s′,a′)−Qt−1(s,a)$\n",
    "\n",
    "The first element is what we get after taking an action and the second element is the previous Q-value.\n",
    "\n",
    "The question is are these two values the same?\n",
    "\n",
    "Ideally, these two values should be the same since the first, $R(s,a)+γmax_a′Q(s′,a′)$ is the formula for calculating $Q(s,a)$.\n",
    "But these values may not be the same because of the randomness that exists in the environment.\n",
    "\n",
    "The reason it's called temporal difference is because of time. We have $Q(s,a)$ which is our previous Q-value and we have our new Q-value, which is $R(s,a)+γmaxa′Q(s′,a′)$.\n",
    "\n",
    "So the question is: has there been a difference between these values in time?\n",
    "\n",
    "Now that we have our temporal difference, here's how we use it:\n",
    "\n",
    "$Qt(s,a)=Qt−1(s,a)+αTDt(a,s)$\n",
    "\n",
    "where:\n",
    "\n",
    "$α$ - is our learning rate.\n",
    "\n",
    "So we take our previous $Qt−1(s,a)$ and add on the temporal difference times the learning rate to get our new $Qt(s,a)$.\n",
    "This equation is how our Q-values are updated over time. Let's now plug in the $TD(a,s)$ equation into our new Q-learning equation:\n",
    "\n",
    "$Qt(s,a)=Qt−1(s,a)+α(R(s,a)+γmax_a′Q(s′,a′)−Qt−1(s,a))$\n",
    "\n",
    "We now have the full Q-learning equation, so let's move on to deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29597c05-0861-4e27-a6b8-02e290aa73a9",
   "metadata": {},
   "source": [
    "### 2.6 Deep Q-Learning Intuition\n",
    "\n",
    "In deep Q-learning we are, of course, making use of neural networks. In terms of the neural network we feed in the state, pass that through several hidden layers (the exact number depends on the architecture) and then output the Q-values. Here is a good visual representation of Q-learning vs. deep Q-learning from Analytics Vidhya:\n",
    "\n",
    "<img src=\"./IMG/Q-learining_vs_Deep_Q-learning.webp\" width=\"600\">\n",
    "\n",
    "You may be wondering why we need to introduce deep learning to the Q-learning equation. Q-learning works well when we have a relatively simple environment to solve, but when the number of states and actions we can take gets more complex we use deep learning as a function approximator. Let's look at how the equation changes with deep Q-learning.\n",
    "\n",
    "Recall the equation for temporal difference:\n",
    "\n",
    "$TD(a,s)=R(s,a)+γmax_a′Q(s′,a′)−Qt−1(s,a)$\n",
    "\n",
    "In the maze example, the neural network will predict 4 values: up, right, left, or down.We then take these 4 values and compare it to the values that were previously predicted, which are stored in memory.\n",
    "\n",
    "So we're comparing Q1 vs. Q−Target1, Q2 vs. Q−Target2, etc.\n",
    "Recall that neural networks work by updating their weights, so we need to adapt our temporal difference equation to leverage this. So what we're going to do is calculate a loss by taking the sum of the squared differences of the Q-values and their targets:\n",
    "\n",
    "$L=∑(Q−Target−Q)^2$.\n",
    "\n",
    "We then take this loss and use backpropagation, or stochastic gradient descent, and pass it through the network and update the weights. This is the learning part, now let's move on to how the agent selects the best action to take.To choose which action is the best, we use the Q-values that we have and pass them through a softmax function. This process happens every time the agent is in a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98f3df-14b5-43c2-ac53-03df8244359b",
   "metadata": {},
   "source": [
    "### 2.7 Experience Replay\n",
    "\n",
    "Now that we've discussed how to apply neural networks to Q-learning, let's review another important concept in deep Q-learning: experience replay.\n",
    "One thing that can lead to our agent misunderstanding the environment is consecutive interdependent states that are very similar.\n",
    "\n",
    "For example, if we're teaching a self-driving car how to drive, and the first part of the road is just a straight line, the agent might not learn how to deal with any curves in the road.\n",
    "This is where experience replay comes in.\n",
    "\n",
    "From our self-driving car example, what happens with experience replay is that the initial experiences of driving in a straight line don't get put through the neural network right away.\n",
    "Instead, these experiences are saved into memory by the agent.\n",
    "Once the agent reaches a certain threshold then we tell the agent to learn from it.\n",
    "\n",
    "So the agent is now learning from a batch of experiences. From these experiences, the agent randomly selects a uniformly distributed sample from this batch and learns from that.\n",
    "\n",
    "Each experience is characterized by that state it was in, the action it took, the state it ended up in, and the reward it received.\n",
    "By randomly sampling from the experiences, this breaks the bias that may have come from the sequential nature of a particular environment, for example driving in a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b965c97-4f32-45c7-917e-4fd56acd0450",
   "metadata": {},
   "source": [
    "### 2.8 Action Selection Policies\n",
    "\n",
    "Now that we understand how deep Q-learning uses experience replay to learn from a batch of experiences stored in memory, let's finish off with how the agent actually selects an action.\n",
    "\n",
    "So the question is: once we have the Q-values, how do decide which one to use?\n",
    "Recall that in simple Q-learning we just choose the action with the highest Q-value.\n",
    "\n",
    "As we mentioned earlier, with deep Q-learning we pass the Q-values through a softmax function.\n",
    "In reality, the action selection policy doesn't need to be softmax, there are others that could be used, and a few of the most common include:\n",
    "\n",
    "* $ϵ-greedy$\n",
    "\n",
    "* $ϵ-soft$\n",
    "\n",
    "* Softmax\n",
    "\n",
    "The reason that we don't just use the highest Q-value comes down to an important concept in reinforcement learning: the exploration vs. exploitation dilemma.\n",
    "\n",
    "To summarize this concept, an agent must make a tradeoff between taking advantage of what it already knows about the environment, or exploring further.\n",
    "If it takes advantage of what it already knows it could gain more rewards, but if it doesn't explore further it may not be choosing the optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889deafe-905d-4be7-8e09-7570ac9d3c71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Implementation of Deep Q-Learning and how to match DeepMind’s score in Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696fb52-23d0-4b8d-8ca5-1af1d0017a53",
   "metadata": {},
   "source": [
    "We are going to explore the method of implementing Deep Q-Networks (DQN) by following the outlined roadmap and explore it problems and resolved them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997113e9-5009-482c-a115-feff9b7ba8bf",
   "metadata": {},
   "source": [
    "python_version() ### 3.1. Reinfocement Learning in Deep Q-Learning\n",
    "\n",
    "In supervised learning, a neural network learns to map inputs to outputs based on labeled training data. For instance, in classifying cats and dogs, the network is trained with labeled images until it can accurately classify new images.\n",
    "\n",
    "In reinforcement learning, the approach is more akin to a child learning a new game. The computer explores the environment, receives occasional rewards when the score increases, and learns to make decisions by evaluating actions that lead to higher scores.\n",
    "\n",
    "In contrast to supervised learning, reinforcement learning (RL) faces challenges with \"sparse\" and \"delayed\" rewards, as mentioned by [Mnih et al. 2013](https://arxiv.org/abs/1312.5602). \"Sparse\" implies infrequent rewards, and \"delayed\" refers to the significant time gap between an action and the corresponding reward. In RL scenarios, the agent might not receive feedback immediately after making a decision, making it challenging to associate actions with their eventual outcomes. For instance, in a maze, choosing a path at a fork may not yield immediate feedback; the reward, finding gold, comes later. To address this, RL uses a discount factor $\\gamma$ to prioritize immediate rewards over delayed ones.\n",
    "\n",
    "The concept introduced by Sutton and Barto in 2018 on page 55 is that of \"discounting\" in the context of reinforcement learning. In this approach, the agent aims to select actions to maximize the sum of the discounted rewards it receives over the future. Specifically, it chooses an action $A_t$ to maximize the \"expected discounted return.\n",
    "\n",
    "The discounted return $G_t$ is calculated as follows  \n",
    "\n",
    "\\begin{align}\n",
    "G_{t} &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... \\\\\n",
    "&=R_{t+1} + \\gamma \\bigl(R_{t+2} + \\gamma R_{t+3} + ...\\bigr)\\\\\n",
    "&= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align}\n",
    "\n",
    "where $R_t$ is the reward the agent received at time $t$.\n",
    "\n",
    "$R_t$ is the reward received at time $t+i$,\n",
    "\n",
    "$\\gamma$ is the discount factor between 0 and 1, and andThe summation represents the accumulation of future rewards, each discounted by a factor of $\\gamma$. \n",
    "\n",
    "This formulation allows the agent to prioritize more immediate rewards over those in the distant future, helping in decision-making processes where the consequences of actions unfold over time.\n",
    "\n",
    "The authors furthermore explain:\n",
    "\n",
    ">a reward received $k$ time steps in the future is worth only $\\gamma^{k−1}$ times what it would be worth if it were received immediately.\n",
    "\n",
    "and\n",
    "\n",
    ">As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.\n",
    "\n",
    "Let us look at a very simple example where there is just one reward not equal to 0 (calculate from right to left):\n",
    "\n",
    "time $t$ |  0| 1 | 2|3|4|\n",
    ":---| --- | ---| ---|---|---|\n",
    "reward sequence $R_t$|  | 0 | 0 | 0 | 1 |\n",
    "discounted returns $G_t$| $\\gamma^3$ | $\\gamma^2$ | $\\gamma$ | 1 | 0 |\n",
    "for $\\gamma=0.9$|0.729 | 0.81 | 0.9| 1 |0|\n",
    "\n",
    "Minor detail: In this example there is no reward at $t=0$ because the first reward $R_1$ is a result of action $a_0$ taken in state $s_0$ and the reward is always associated with the next state in Sutton and Barto. Some articles associate the reward $R_i$ with the state $s_i$ and action $a_i$ that caused the reward. In that case the definition of the discounted return changes to $G_t=R_{t} + \\gamma G_{t+1}$, in the table the 1's would all be in the same column and there would be a reward equal to 0 at $t=0$ but not at $t=4$ .\n",
    "\n",
    "Simply put, by discounting returns, future rewards increase past or current returns and the closer $\\gamma$ is to 1, the farther the agent can *see* into the future. \n",
    "\n",
    "And *sparse*?\n",
    "In our fictive maze example, the rewards are the sparser, the less gold you find. For an agent, a game is more difficult to learn, the sparser the reward is. \"Pong\" is one of the games DQN can learn fastest because the score changes quite often. \"Montezuma's Revenge\", on the other hand, has very sparse rewards and DQN (at least without some additional tricks) is not able to learn the game at all.\n",
    "\n",
    "### 3.2 Q-learning in Deep Q-learning\n",
    "\n",
    "\n",
    "So how does Q-learning work? If the agent (regardless if trained or still untrained) is shown a state $s$ of the game, it has to decide which action $a$ to perform (for example move paddle left or right in breakout). How does it do that? On page 2 [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) define the so-called $Q$-Function:\n",
    "\n",
    ">We define the optimal action-value function $Q^∗(s, a)$ as the maximum expected return achievable by following any strategy, after seeing some sequence $s$ and then taking some action $a$ \n",
    "\n",
    "This means that given a state of the game $s$ (for now please consider *sequences* as states of the game), $Q^*(s,a)$ is the best (discounted) total return the agent can achieve if it performs action $a$ in the current state $s$. So how does it choose which action to perform assuming we already know $Q^*(s,a)$? One obvious strategy would be to always choose the action with the maximum value of $Q^*$ (we will see later, why this is slightly problematic). But first of all, we need to find this magical function $Q^*$:\n",
    "\n",
    "Let's say we are in state $s$, decide to perform action $a$, and arrive in the next state $s'$. If we assume that in state $s'$ the $Q^*$-values for all possible actions $a'$ were already known, then the $Q^*$-value in state $s$ for action $a$ (action-value in $s$ for action $a$) would be the reward $r$ we got for performing action $a$ plus the discounted maximum future return in $s'$:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^*(s,a) = r + \\gamma \\textrm{max}\\left( Q^*(s',a') \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Less formal but more intuitive:\n",
    "If we were at a certain fork in the maze (state $s$) and want to know how good it was to choose the left path (perform action $a$), we add the gold we found after going left (reward we received for performing action $a$) and the amount of gold we expect to find down the road (maximum future discounted return). If you have to choose between finding a golden coin immediately after going left at a fork but nothing else down the road or finding nothing immediatley after going right but a treasure down the road, well, the right path is worth more/has a higher action-value.\n",
    "\n",
    "This is the so-called **Bellman equation**. Deep Q-Learning uses a neural network to find an approximation $Q(s,a,\\theta)$ of $Q^*(s,a)$. $\\theta$ are the parameters of the neural network. We will discuss later, how exactly the parameters of the network are updated. Now, I will explain, how the neural network maps a state $s$ to $Q$-values for the possible actions $a$.\n",
    "\n",
    "Earlier I mentioned, that I regard a *sequence* as a *state*. What did I mean with that? Imagine you have a pin-sharp image of a flying soccer ball. Can you tell in which direction it moves? No, you cannot, but you could if there was some kind of motion blur in the picture or if you had several images taken quickly one after another showing the ball in a slightly different position every time. The same problem occurs in Atari games. From a single frame of the game \"Pong\", the agent can not discern in which direction the ball moves. DeepMind met this problem by stacking several consecutive frames and considering this sequence a state that is passed to the neural network. From such a sequence the agent is able to detect the direction and speed of movement because the ball is in a different position in each frame.\n",
    "\n",
    ">Since the agent only observes images of the current screen [...] it is impossible to fully understand the current situation from only the current screen $x_t$. We therefore consider sequences of actions and observations, $s_t = x_1, a_1, x_2, ..., a_{t−1}, x_t$, and learn game strategies that depend upon these sequences. All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state. ([page 2 of Mnih et al. 2013](https://arxiv.org/abs/1312.5602))\n",
    "\n",
    "\n",
    "On page 5 of [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) the authors explain the preprocessing of the frames:\n",
    "\n",
    ">Working directly with raw Atari frames, which are 210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to gray-scale and down-sampling it to a 110×84 image. The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area. The final cropping stage is only required because we use the GPU implementation of 2D convolutions from [...], which expects square inputs. For the experiments in this paper, the function $\\phi$ [...] applies this preprocessing to the last 4 frames of a history and stacks them to produce the input to the $Q$-function.\n",
    "\n",
    "So let us start by looking at how the preprocessing can be implemented. I used `gym` from OpenAi to provide the environment. A frame returned by the environment has the shape `(210,160,3)` where the 3 stands for the RGB color channels. Such a frame is passed to the Functor`FrameProcessor` which transforms it to a `(84,84,1)` frame, where the 1 indicates that instead of three RGB channels there is one grayscale channel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5420d876-90ab-45df-85ee-c303dc24eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "        \"\"\"\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, \n",
    "                                                [self.frame_height, self.frame_width], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55282f3e-8e67-49a1-b7fd-c7c0f1c732c5",
   "metadata": {},
   "source": [
    "## 3. Dueling Networks\n",
    "\n",
    "Instead of the network architecture described in [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) or [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) I used the dueling network architecture described in [Wang et al. 2016](https://arxiv.org/abs/1511.06581).\n",
    "\n",
    "<img src=\"./IMG/dueling.png\" width=\"600\">\n",
    "\n",
    "Both the [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) and the [Wang et al. 2016](https://arxiv.org/abs/1511.06581) dueling architecture have the same low-level convolutional structure:\n",
    "\n",
    ">The first convolutional layer has 32 8x8 filters with stride 4, the second 64 4x4 filters with stride 2, and the third and final convolutional layer consists 64 3x3 filters with stride 1.\n",
    "\n",
    "In the normal DQN architecture (top network in the figure) the *final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.* (see page 6 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/)) \n",
    "\n",
    "These outputs are the predicted $Q(s,a;\\theta)$-values for action $a$ in state $s$.\n",
    "\n",
    "Instead of directly predicting a single $Q$-value for each action, the dueling architecture splits the final convolutional layer into two streams that represent the value and advantage functions that predict a *state value* $V(s)$ that depends only on the state, and *action advantages* $A(s,a)$ that depend on the state and the respective action. On page 2 of [Wang et al. 2016](https://arxiv.org/abs/1511.06581) the authors explain:\n",
    "\n",
    ">Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way. \n",
    "In the experiments, we demonstrate that the dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problem. \n",
    "\n",
    "The *state value* $V(s)$ predicts *how good it is to be in a certain state* $s$ and the *action advantage* $A(s,a)$ predicts *how good it is to perform action $a$ in state $s$*.\n",
    "I suggest you take a look at figure 2 in [Wang et al. 2016](https://arxiv.org/abs/1511.06581) to better understand what the value- and advantage-stream learn to look at.\n",
    "\n",
    "Next, we have to combine the value and advantage stream into $Q$-values $Q(s,a)$. This is done the following way (equation 9 in [Wang et al. 2016](https://arxiv.org/abs/1511.06581)):\n",
    "\n",
    "\\begin{equation}\n",
    "Q(s,a) = V(s) + \\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Why so complicated instead of just adding $V(s)$ and $A(s,a)$? Let's assume $Q(s,a) = V(s) + A(s,a)$:\n",
    "\n",
    " The Q function measures the value of choosing a particular action when in a particular state. The value function $V$, which is the expected value of $Q$ over all possible actions, $V = E(Q)$, measures how good it is to be in this particular state. \n",
    "\n",
    "If you combine $E(Q) = V$ and $Q = V + A$, you find $E(Q) = E(V) + E(A)$. But $V$ does not depend on any action, which means $E(V)=V$, $E(Q) = V + E(A) = V$ and thus $E(A)=0$. The expected value of the advantage $A(s,a')$ over all possible actions $a'$ has to be zero. The expected value of the expression in parentheses is zero because we subtract the mean of the advantages from every advantage:\n",
    "\n",
    "\\begin{equation}\n",
    "E\\left(A(s,a) - \\frac 1{| \\mathcal A |}\\sum_{a'}A(s, a')\\right) = E\\Bigl(A - E(A)\\Bigr) = E(A)- E(E(A)) = E(A) - E(A) = 0\n",
    "\\end{equation}\n",
    "\n",
    "In the cell below you find the code that implements this architecture in tensorflow. Some things to keep in mind: You should normalize the input pixel values to [0,1] by dividing the input with 0xFF=255. The reason for this is, that the pixel values of the frames, the environment returns, are uint8 which can store values in the range [0,255]. \n",
    "\n",
    "**Furthermore, make sure you initialize the weights properly! \n",
    "The DQN uses the Relu activation function and the right initializer is [He et al. 2015 equation 10](https://arxiv.org/pdf/1502.01852v1.pdf) ([click here for a detailed explanation](https://www.youtube.com/watch?v=s2coXdufOzE&t=157s )).\n",
    "In tensorflow use `tf.variance_scaling_initializer` [(documentation)](https://www.tensorflow.org/api_docs/python/tf/variance_scaling_initializer) with `scale = 2`.**\n",
    "\n",
    "DeepMind used an implementation of the RMSProp optimizer that is different to the one in tensorflow (see page 23, eq 40 in [Graves 2014](https://arxiv.org/pdf/1308.0850v5.pdf)). Before implementing it myself, I tried the Adam optimizer which gave promising results without much hyperparameter-search. Adam was not invented when [Mnih et al. 2013](https://arxiv.org/abs/1312.5602) was published, so one could argue that they might have used it instead of RMSProp if it had been invented earlier. On the other hand, the authors of this [blog post](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/) compare *Momentum, RMSProp and Adam* and argue:\n",
    ">Out of the above three, you may find momentum to be the most prevalent, despite Adam looking the most promising on paper. Empirical results have shown that all these algorithms can converge to different optimal local minima given the same loss. However, SGD with momentum seems to find more flatter minima than Adam, while adaptive methods tend to converge quickly towards sharper minima. Flatter minima generalize better than sharper ones.\n",
    "\n",
    "Maybe some of these differences might be mitigated by the use of *AdamW*, which I described [in this blog post](https://medium.com/@fabiograetz/why-adamw-matters-736223f31b5d) (irrelevant for DQN since no regularization is used). For now, I stick with Adam and if I find some time in the future, I might come back to this since it might be well worth spending some time on playing with different optimizers and implementing the version of RMSProp used by DeepMind. \n",
    "\n",
    "*Edit*: In a later DeepMind paper called \"Rainbow: Combining Improvements in Deep Reinforcement Learning\" by [Hessel et al. 2017](https://arxiv.org/abs/1710.02298) RMSProp was substituted for Adam with a learning rate of 0.0000625 (see Table 1). This learning rate is close to what I found working well for Breakout (0.00001) before reading Hessel et al. 2017.\n",
    "\n",
    "If you compare the dueling architecture described above to the network implemented in the next cell, you will find a small difference. Instead of two hidden fully connected layers with 512 rectifier units for each, the value and the advantage stream, I ended up adding a fourth convolutional layer with 1024 filters. The output has the shape (1, 1, 1024) and is then split into two streams with shapes (1, 1, 512). This architecture is suggested [here](https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb) and after performing some tests on the environment Pong, which is comparably easy to learn for a DQN agent, I find that this small adjustment lets the reward increase slightly earlier and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7bf1f-bf9c-4b27-a33a-7352d0460d29",
   "metadata": {},
   "source": [
    "# Refferences\n",
    "[1] How to match DeepMind’s Deep Q-Learning score in Breakout by Fabio M. Graetz, Aug 26, 2018\n",
    "\n",
    "[2] Deep Reinforcement Learning: Guide to Deep Q-Learning by Peter Foy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
